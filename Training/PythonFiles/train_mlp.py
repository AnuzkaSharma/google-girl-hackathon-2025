import os
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error

# ----------------------------------------------
# âœ… Step 1: Load Dataset Function
# ----------------------------------------------
def get_dataset_path():
    """Returns the correct dataset path."""
    dataset_path = "D:/Anushka/Projects/Google Girl Hackathon/Training/CSV Files/feature_data.csv"

    if not os.path.exists(dataset_path):
        print(f"âŒ Error: Dataset not found at {dataset_path}!")
        return None

    print(f"ğŸ“‚ Dataset found at: {dataset_path}")
    return dataset_path

def load_data():
    """Loads the dataset from the fixed path."""
    dataset_path = get_dataset_path()
    if dataset_path is None:
        return None

    try:
        df = pd.read_csv(dataset_path)
        print(f"âœ… Successfully loaded dataset from {dataset_path}")
        return df
    except Exception as e:
        print(f"âŒ Error loading dataset: {e}")
        return None

# ----------------------------------------------
# âœ… Step 2: Data Preprocessing
# ----------------------------------------------
def preprocess_data(df, target_column):
    """Preprocess dataset: Extract features/target and normalize input."""
    if df is None:
        print("âŒ Error: No dataset found!")
        return None, None, None

    print("ğŸ”„ Preprocessing dataset...")

    # âœ… Remove non-numeric columns if any
    non_numeric_cols = ["Filename", "Folder"]
    df = df.drop(columns=[col for col in non_numeric_cols if col in df.columns], errors="ignore")

    # âœ… Check if target column exists
    if target_column not in df.columns:
        print(f"âŒ Error: Target column '{target_column}' not found in dataset!")
        return None, None, None

    X = df.drop(columns=[target_column])
    y = df[target_column]

    # âœ… Check for extreme values before scaling
    print(f"ğŸ” Checking dataset values: Max={X.max().max()}, Min={X.min().min()}")

    # âœ… Normalize Features for Neural Network
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # âœ… After Scaling, Check Values
    print(f"ğŸ” After Scaling: Max={X_scaled.max()}, Min={X_scaled.min()}")

    return X_scaled, y, scaler

# ----------------------------------------------
# âœ… Step 3: Train MLP Neural Network with Hyperparameter Tuning
# ----------------------------------------------
def train_mlp(X, y, scaler, model_save_path="D:/Anushka/Projects/Google Girl Hackathon/Training/saved_models/best_mlp.pkl"):
    """Trains an MLP Neural Network with hyperparameter tuning and saves it."""
    print("ğŸš€ Initializing MLP Neural Network model with tuning...")

    # âœ… Train-Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # âœ… Define Base MLP Model
    mlp_model = MLPRegressor(max_iter=1000, random_state=42)

    # âœ… Define Hyperparameter Grid for RandomizedSearchCV
    param_dist = {
        "hidden_layer_sizes": [(64, 32), (128, 64, 32), (256, 128, 64)],
        "activation": ["relu", "tanh"],
        "solver": ["adam"],  # ğŸ‘ˆ "sgd" hata diya kyunki instability la raha tha
        "learning_rate_init": [0.001, 0.01],
        "alpha": [0.0001, 0.001, 0.01]
    }

    # âœ… Use RandomizedSearchCV for Tuning
    random_search = RandomizedSearchCV(
        mlp_model, param_distributions=param_dist, 
        n_iter=10, cv=3, scoring="neg_mean_squared_error", 
        random_state=42, n_jobs=-1
    )

    print("ğŸ¯ Running Hyperparameter Tuning...")
    random_search.fit(X_train, y_train)

    # âœ… Best Model from Tuning
    best_mlp = random_search.best_estimator_
    best_params = random_search.best_params_
    print(f"ğŸ† Best Parameters Found: {best_params}")

    # âœ… Model Evaluation
    print("ğŸ“Š Evaluating Best MLP Model...")
    y_train_pred = best_mlp.predict(X_train)
    y_test_pred = best_mlp.predict(X_test)

    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)

    print(f"âœ… Training MSE: {train_mse:.4f}")
    print(f"âœ… Testing MSE: {test_mse:.4f}")

    # âœ… Ensure folder exists before saving model
    model_dir = os.path.dirname(model_save_path)
    os.makedirs(model_dir, exist_ok=True)

    # âœ… Save Best Model & Scaler
    try:
        joblib.dump({"model": best_mlp, "scaler": scaler}, model_save_path)
        print(f"ğŸ’¾ Best tuned MLP model saved at: {model_save_path}")
    except Exception as e:
        print(f"âŒ ERROR while saving model: {e}")

    return best_mlp, {"train_mse": train_mse, "test_mse": test_mse}

# ----------------------------------------------
# âœ… Step 4: Run the Training Process
# ----------------------------------------------
if __name__ == "__main__":
    print("ğŸ“¥ Loading dataset for MLP Neural Network training...")
    df = load_data()

    if df is not None:
        target_column = "Estimated Depth"  # ğŸ‘ˆ Make sure this column exists in CSV
        X, y, scaler = preprocess_data(df, target_column)

        if X is not None and y is not None:
            best_mlp, results = train_mlp(X, y, scaler)
            print("\nğŸ† Model Training & Tuning Completed!", results)
        else:
            print("âŒ Training aborted due to preprocessing failure.")
    else:
        print("âŒ Training aborted due to missing dataset.")
